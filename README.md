# IEOR 4573 Capstone Project

Analyze unstructured customer reviews with LLMs. Perform sentiment classification and aspect-based extraction (e.g., shipping, pricing, quality, support) to surface strengths and pain points. Summarize insights and visualize trends in an interactive dashboard for actionable business decisions.

## Prerequisites

Download Amazon customer review datasets and store them in `./data/` directory
  - [Ebook](https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset/data?select=amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv)
  - [Music](https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset/data?select=amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv)
  - [Software](https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset/data?select=amazon_reviews_us_Digital_Software_v1_00.tsv)
  - [Videos](https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset/data?select=amazon_reviews_us_Digital_Video_Download_v1_00.tsv)
  - [Video Games](https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset/data?select=amazon_reviews_us_Digital_Video_Games_v1_00.tsv)

## Installation

1. (Optional) Create and activate a virtual environment:
	```powershell
	python -m venv .venv
	.\.venv\Scripts\activate
	```

2. Install all required packages:
	```powershell
	pip install -r requirements.txt
	```
3. Set your OpenAI API key (do NOT share this key or commit it to the repo):

	 - **Option 1: Environment variable (recommended)**
		 - In PowerShell:
			 ```powershell
			 $env:OPENAI_API_KEY = "your-api-key-here"
			 ```
		 - In bash:
			 ```bash
			 export OPENAI_API_KEY="your-api-key-here"
			 ```

	 - **Option 2: .env file**
		 - Copy `.env.example` to `.env` and add your key:
			 ```
			 cp .env.example .env
			 # Then edit .env and set your key
			 ```

Each user should use their own API key from the OpenAI dashboard. Never commit your real key to the repository.

## Project Structure

- `data/` (local, user-provided): Place the raw Amazon TSV files here. This folder is typically git-ignored.
- `preprocessed_data/`: Intermediate and final datasets produced by the pipeline
  - `final_dataset.csv`: Consolidated dataset across categories
- `llm_analyzed_data_output/`: Outputs generated by LLM prompts over sampled reviews
  - `prompt1_processed_reviews.csv`
  - `prompt2_processed_reviews.csv`
  - `prompt3_processed_reviews.csv`
- `prompts/`: Prompt templates used in the LLM analysis notebooks
  - `prompt1.txt`, `prompt2.txt`, `prompt3.txt`
- `utils/`: Reusable library code
  - `helpers/data_loader.py`: Load and clean TSV reviews; dtype conversions and basic validation
  - `helpers/sampling.py`: Word-count features and stratified sampling by month/length (and optional rating)
  - `helpers/summary.py`: Dataset validation summaries and quick stats helpers
- `data_pipeline.py`: Unified pipeline (`ReviewDataPipeline`) to process categories, sample reviews, and merge outputs
- `pipeline.ipynb`: Interactive walkthrough of the end-to-end data pipeline
- `prompt1_llm_sentiment_analysis.ipynb`, `prompt2_llm_sentiment_analysis.ipynb`: Notebooks to run LLM-based analysis with the prompts
- `requirements.txt`: Python dependencies

## Typical workflow

1. Download TSVs and place them in `data/`.
2. Use `pipeline.ipynb` or `data_pipeline.py` to:
   - Load, clean, and filter reviews (e.g., to a target year)
   - Add length features and perform stratified sampling
   - Export per-category CSVs to `preprocessed_data/` and merge into `preprocessed_data/final_dataset.csv`
3. Run LLM notebooks to generate analysis outputs in `llm_analyzed_data_output/` using prompts in `prompts/`.
